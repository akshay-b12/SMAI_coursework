{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ns006gZItu8t"
   },
   "source": [
    "## Q-2 : Multi-class logistic regression\n",
    "\n",
    "*   Akshay Bankar (2019201011)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Softmax regression, also called multinomial logistic regression extends logistic regression to multiple classes.\n",
    "\n",
    "**Given:** \n",
    "- dataset $\\{(\\boldsymbol{x}^{(1)}, y^{(1)}), ..., (\\boldsymbol{x}^{(m)}, y^{(m)})\\}$\n",
    "- with $\\boldsymbol{x}^{(i)}$ being a $d-$dimensional vector $\\boldsymbol{x}^{(i)} = (x^{(i)}_1, ..., x^{(i)}_d)$\n",
    "- $y^{(i)}$ being the target variable for $\\boldsymbol{x}^{(i)}$, for example with $K = 3$ classes we might have $y^{(i)} \\in \\{0, 1, 2\\}$\n",
    "\n",
    "A softmax regression model has the following features: \n",
    "- a separate real-valued weight vector $\\boldsymbol{w}= (w^{(1)}, ..., w^{(d)})$ for each class. The weight vectors are stored as rows in a weight matrix.\n",
    "- a separate real-valued bias $b$ for each class\n",
    "- the softmax function as an activation function\n",
    "- the cross-entropy loss function\n",
    "\n",
    "An illustration of the whole procedure is given below.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1KnsKr7sPU82TcO6V5cwVMw0WEOrEqGGU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZgeaPTFv-xy"
   },
   "source": [
    "#### Training steps of softmax regression model :\n",
    "* * * \n",
    "**Step 0:** Initialize the weight matrix and bias values with zeros (or small random values).\n",
    "* * *\n",
    "\n",
    "**Step 1:** For each class $k$ compute a linear combination of the input features and the weight vector of class $k$, that is, for each training example compute a score for each class. For class $k$ and input vector $\\boldsymbol{x}^{(i)}$ we have:\n",
    "\n",
    "$score_{k}(\\boldsymbol{x}^{(i)}) = \\boldsymbol{w}_{k}^T \\cdot \\boldsymbol{x}^{(i)} + b_{k}$\n",
    "\n",
    "where $\\cdot$ is the dot product and $\\boldsymbol{w}_{(k)}$ the weight vector of class $k$.\n",
    "We can compute the scores for all classes and training examples in parallel, using vectorization and broadcasting:\n",
    "\n",
    "$\\boldsymbol{scores} = \\boldsymbol{X} \\cdot \\boldsymbol{W}^T + \\boldsymbol{b} $\n",
    "\n",
    "where $\\boldsymbol{X}$ is a matrix of shape $(n_{samples}, n_{features})$ that holds all training examples, and $\\boldsymbol{W}$ is a matrix of shape $(n_{classes}, n_{features})$ that holds the weight vector for each class. \n",
    "* * *\n",
    "\n",
    "**Step 2:** Apply the softmax activation function to transform the scores into probabilities. The probability that an input vector $\\boldsymbol{x}^{(i)}$ belongs to class $k$ is given by\n",
    "\n",
    "$\\hat{p}_k(\\boldsymbol{x}^{(i)}) = \\frac{\\exp(score_{k}(\\boldsymbol{x}^{(i)}))}{\\sum_{j=1}^{K} \\exp(score_{j}(\\boldsymbol{x}^{(i)}))}$\n",
    "\n",
    "Again we can perform this step for all classes and training examples at once using vectorization. The class predicted by the model for $\\boldsymbol{x}^{(i)}$ is then simply the class with the highest probability.\n",
    "* * *\n",
    "\n",
    "**Step 3:** Compute the cost over the whole training set. We want our model to predict a high probability for the target class and a low probability for the other classes. This can be achieved using the cross entropy loss function: \n",
    "\n",
    "$J(\\boldsymbol{W},b) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^{K} \\Big[ y_k^{(i)} \\log(\\hat{p}_k^{(i)})\\Big]$\n",
    "\n",
    "In this formula, the target labels are *one-hot encoded*. So $y_k^{(i)}$ is $1$ is the target class for $\\boldsymbol{x}^{(i)}$ is k, otherwise $y_k^{(i)}$ is $0$.\n",
    "* * *\n",
    "\n",
    "**Step 4:** Compute the gradient of the cost function with respect to each weight vector and bias.\n",
    "\n",
    "The general formula for class $k$ is given by:\n",
    "\n",
    "$ \\nabla_{\\boldsymbol{w}_k} J(\\boldsymbol{W}, b) = \\frac{1}{m}\\sum_{i=1}^m\\boldsymbol{x}^{(i)} \\left[\\hat{p}_k^{(i)}-y_k^{(i)}\\right]$\n",
    "\n",
    "For the biases, the inputs $\\boldsymbol{x}^{(i)}$ will be given 1.\n",
    "* * *\n",
    "\n",
    "**Step 5:** Update the weights and biases for each class $k$:\n",
    "\n",
    "$\\boldsymbol{w}_k = \\boldsymbol{w}_k - \\eta \\, \\nabla_{\\boldsymbol{w}_k} J$  \n",
    "\n",
    "$b_k = b_k - \\eta \\, \\nabla_{b_k} J$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALRoGaqztu8w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from MyPCA import MyPCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define class for multiclass logistic regression with the steps defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVYoBBfAtu8_"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learn_rate = 0.001, num_iters = 100):\n",
    "        self.learning_rate = learn_rate\n",
    "        self.n_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        self.data = self.add_bias_col(data)\n",
    "        self.n_samples, self.n_features = self.data.shape \n",
    "        self.classes = np.unique(labels)\n",
    "        self.class_labels = {c:i for i,c in enumerate(self.classes)}\n",
    "        labels = self.one_hot_encode(labels)\n",
    "        self.weights = np.zeros(shape=(len(self.classes),self.data.shape[1]))\n",
    "        for _ in range(self.n_iters):\n",
    "            y = np.dot(self.data, self.weights.T).reshape(-1,len(self.classes)) ## y = m*x + c\n",
    "            ## apply softmax\n",
    "            y_predicted = self.softmax(y)\n",
    "            #y_predicted = self.sigmoidfn(y)\n",
    "            \n",
    "            # compute gradients\n",
    "            dw = np.dot((y_predicted - labels).T, self.data)\n",
    "            # update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "        #print(self.weights)\n",
    "    \n",
    "    def add_bias_col(self,X):\n",
    "        return np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    def one_hot_encode(self, y):\n",
    "        return np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "    '''\n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1,1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.add_bias_col(X)\n",
    "        pred_vals = np.dot(X, self.weights.T).reshape(-1,len(self.classes))\n",
    "        self.probs_ = self.softmax(pred_vals)\n",
    "        pred_classes = np.vectorize(lambda c: self.classes[c])(np.argmax(self.probs_, axis=1))\n",
    "        return pred_classes\n",
    "        #return np.mean(pred_classes == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the data images and perform PCA using the class defined in Q-1.\n",
    "\n",
    "> The input images are converted to grayscale and resized to (64,64).\n",
    "\n",
    "> Number of PCA components corresponding to 95% of variance are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bi6b05Nhtu9N",
    "outputId": "09b0409a-822d-407c-a7f2-959df7dc4bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data transformed after performing PCA : (520, 137)\n"
     ]
    }
   ],
   "source": [
    "def read_data(path):\n",
    "        img_files = glob.glob(path)\n",
    "        #print(img_files)\n",
    "        gray_images = []\n",
    "        labels = []\n",
    "        for file in img_files:\n",
    "            img = cv2.imread(file)\n",
    "            img = cv2.resize(img,(64,64),interpolation=cv2.INTER_AREA) #None,fx=0.5,fy=0.5\n",
    "            flat_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).flatten()\n",
    "            gray_images.append(flat_img)\n",
    "            lab = ((file.split('/')[-1]).split('_')[0]).lstrip('0')\n",
    "            if not lab:\n",
    "                labels.append(0)\n",
    "            else :\n",
    "                labels.append(int(lab))\n",
    "        return np.asarray(gray_images), labels\n",
    "    \n",
    "data, labels = read_data(\"./dataset/*\")\n",
    "pca = MyPCA(n_components = 0.95)#n_components = 0.95\n",
    "pca_data = pca.fit(data)\n",
    "print(\"Shape of data transformed after performing PCA :\",pca_data.shape)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLR_E3O7tu9X",
    "outputId": "a1ea7a9b-ede5-49bf-b14a-eea67f2cfb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data : (416, 137)\n",
      "Shape of test data : (104, 137)\n",
      "Confusion-matrix :\n",
      "[[ 9  0  3  0  0  0  1  0]\n",
      " [ 2  8  1  0  0  0  0  0]\n",
      " [ 0  0 15  0  0  0  0  0]\n",
      " [ 0  1  0  7  1  0  0  1]\n",
      " [ 0  0  0  1  9  2  0  0]\n",
      " [ 0  0  3  0  1  9  0  0]\n",
      " [ 0  0  2  2  0  0  8  1]\n",
      " [ 0  0  0  1  1  0  0 15]]\n",
      "Classification-report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.69      0.75        13\n",
      "           1       0.89      0.73      0.80        11\n",
      "           2       0.62      1.00      0.77        15\n",
      "           3       0.64      0.70      0.67        10\n",
      "           4       0.75      0.75      0.75        12\n",
      "           5       0.82      0.69      0.75        13\n",
      "           6       0.89      0.62      0.73        13\n",
      "           7       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.77       104\n",
      "   macro avg       0.79      0.76      0.76       104\n",
      "weighted avg       0.79      0.77      0.77       104\n",
      "\n",
      "Accuracy score : 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(pca_data, labels, train_size=0.8, random_state=666)\n",
    "print(\"Shape of train data :\",np.shape(train_X))\n",
    "print(\"Shape of test data :\", np.shape(test_X))\n",
    "logreg = LogisticRegression()\n",
    "logreg.train(np.asarray(train_X), np.asarray(train_y))\n",
    "pred_labels = logreg.predict(np.asarray(test_X))\n",
    "#print(\"Accuracy : \",, np.asarray(test_y)))\n",
    "\n",
    "print (\"Confusion-matrix :\")\n",
    "print(confusion_matrix(test_y,pred_labels))\n",
    "print(\"Classification-report\")\n",
    "print (classification_report(test_y,pred_labels))\n",
    "print (\"Accuracy score :\", accuracy_score(test_y,pred_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smai_a3_q2_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
